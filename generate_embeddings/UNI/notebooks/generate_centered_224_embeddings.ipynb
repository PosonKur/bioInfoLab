{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this tutorial, we will use the **UNI model** from [MahmoodLab](https://github.com/mahmoodlab/UNI) (follow the Installation steps), a self-supervised vision transformer trained on histology slides, to extract **patch-level feature embeddings** from Whole Slide Images (WSIs).\n",
    "\n",
    "The steps we will follow are:\n",
    "\n",
    "1. **Load** the pretrained UNI model and its checkpoint weights.\n",
    "2. **Tokenize** the WSI into small, non-overlapping patches of size **224×224 pixels**.\n",
    "3. **Save** each patch image and record its spatial location (X, Y coordinates) on the original WSI.\n",
    "4. **Feed** the patches through the pretrained UNI model to extract deep **feature embeddings**.\n",
    "5. **Link** each embedding with its spatial metadata (Slide ID, Patch ID, coordinates).\n",
    "6. **Save** the final dataset for downstream spatial analysis tasks.\n",
    "\n",
    "---\n",
    "\n",
    "By following this process, we create a compact and meaningful representation of very large WSIs, which can be easily used for downstream tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Environment Setup\n",
    "\n",
    "In this section, we import the necessary libraries and set up the environment.\n",
    "\n",
    "- **PyTorch** (`torch`) and **TorchVision**: for deep learning operations and image handling.\n",
    "- **Pandas**, **NumPy**: for working with tabular data and arrays.\n",
    "- **PIL** (`Image`): for opening and processing images.\n",
    "- **Tifffile**: for reading `.tiff` format images, commonly used in digital pathology.\n",
    "- **UNI model utilities**: \n",
    "  - `get_encoder` to load the pretrained UNI model.\n",
    "  - `extract_patch_features_from_dataloader` to extract embeddings from patches.\n",
    "  - Various evaluation utilities for downstream analysis (e.g., linear probe, k-NN, ProtoNet).\n",
    "\n",
    "Finally, we set the **device** to use a **GPU** if available, otherwise fallback to **CPU**:\n",
    "```python\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgr/anaconda3/envs/UNI/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "from os.path import join as j_\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tifffile\n",
    "from uni import get_encoder\n",
    "import tifffile\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from uni.downstream.extract_patch_features import extract_patch_features_from_dataloader\n",
    "from uni.downstream.eval_patch_features.linear_probe import eval_linear_probe\n",
    "from uni.downstream.eval_patch_features.fewshot import eval_knn, eval_fewshot\n",
    "from uni.downstream.eval_patch_features.protonet import ProtoNet, prototype_topk_vote\n",
    "from uni.downstream.eval_patch_features.metrics import get_eval_metrics, print_metrics\n",
    "from uni.downstream.utils import concat_images\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input and Output setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"centered_224\" #Method to extract the patches\n",
    "tissue = \"adenocarcinoma\" #Type of tissue\n",
    "\n",
    "wsi_folder = f\"imgs/{tissue}/\" #Where the img is located\n",
    "output_base_folder = f\"{method}/Patches/\" #Where you want to save the patches\n",
    "embedding_file = f\"{method}/{tissue}/WSI_patch_embeddings.csv\" #name of the file with the embeddings\n",
    "metadata_file = f\"{method}/{tissue}/WSI_patch_metadata.csv\"   #name of the fiel(optional)\n",
    "local_dir = 'checkpoint_path' #Where the model is located\n",
    "\n",
    "os.makedirs(f\"{method}\", exist_ok=True)\n",
    "os.makedirs(f\"{method}/{tissue}\", exist_ok=True)\n",
    "os.makedirs(f\"{method}/Patches\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Pretrained UNI Model and Preparing Image Transformations\n",
    "\n",
    "In this section, we load the **pretrained UNI model** and prepare the **transformations** that will be applied to the image patches before feeding them into the model.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. **Import additional libraries**:\n",
    "   - `timm`: a popular library that provides implementations of vision transformer models.\n",
    "   - `huggingface_hub`: for downloading model weights (optional here if already downloaded).\n",
    "\n",
    "2. **Set the local directory**:\n",
    "   - `local_dir` is where the pretrained model checkpoint (`pytorch_model.bin`) is stored. Here 'checkpoint_path' is an exampl, set where your checkpoint is located.\n",
    "   - (Optional) If not downloaded yet, you could use `hf_hub_download` to fetch the weights from Hugging Face.\n",
    "\n",
    "3. **Model setup**:\n",
    "   - We define the model architecture using `timm_kwargs` to match the configuration used by UNI (based on a Vision Transformer (ViT) backbone).\n",
    "   - The model is created with `timm.create_model(**timm_kwargs)`.\n",
    "   - Pretrained weights are loaded with `model.load_state_dict(...)`.\n",
    "   - The model is set to evaluation mode (`model.eval()`) and moved to the selected device (CPU or GPU).\n",
    "\n",
    "4. **Image preprocessing**:\n",
    "   - Before passing patches to the model, we apply a series of transformations:\n",
    "     - **Resize** the patch to 224×224 pixels.\n",
    "     - **Convert** the image to a PyTorch tensor.\n",
    "     - **Normalize** pixel values using ImageNet statistics (mean and std).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "from huggingface_hub import login, hf_hub_download\n",
    "\n",
    "#login()  \n",
    "\n",
    "#os.makedirs(local_dir, exist_ok=True)  \n",
    "#hf_hub_download(\"MahmoodLab/UNI2-h\", filename=\"pytorch_model.bin\", local_dir=local_dir, force_download=True)\n",
    "timm_kwargs = {'model_name': 'vit_giant_patch14_224',\n",
    "    'img_size': 224, \n",
    "               'patch_size': 14, \n",
    "               'depth': 24,\n",
    "               'num_heads': 24,\n",
    "               'init_values': 1e-5, \n",
    "               'embed_dim': 1536,\n",
    "               'mlp_ratio': 2.66667*2,\n",
    "               'num_classes': 0, \n",
    "               'no_embed_class': True,\n",
    "               'mlp_layer': timm.layers.SwiGLUPacked, \n",
    "               'act_layer': torch.nn.SiLU, \n",
    "               'reg_tokens': 8, \n",
    "               'dynamic_img_size': True\n",
    "              }\n",
    "model = timm.create_model(**timm_kwargs)\n",
    "model.load_state_dict(torch.load(os.path.join(local_dir, \"pytorch_model.bin\"), map_location=\"cpu\"), strict=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Whole Slide Images (WSIs) into Patches\n",
    "\n",
    "In this section, we split each Whole Slide Image (WSI) into small non-overlapping patches and save them as individual image files, along with recording their spatial metadata.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. **Set up paths**:\n",
    "   - `wsi_folder`: location of original WSI images (in `.tif` format). (set your location)\n",
    "   - `output_base_folder`: where extracted patches will be saved. (set your location)\n",
    "   - `metadata_file`: CSV file to store patch metadata (Slide_ID, Patch_ID, X, Y, file path). (set your file name and location or keep it as the default)\n",
    "\n",
    "2. **Patch parameters**:\n",
    "   - **Patch size**: `224 × 224` pixels.\n",
    "   - **Stride**: `224` pixels (non-overlapping patches).\n",
    "\n",
    "3. **Processing each WSI**:\n",
    "   - Load the `.tif` file using `tifffile.imread()`.\n",
    "   - Convert the numpy array to a PIL image for easier patch extraction.\n",
    "   - Loop over the WSI grid and **crop** patches at regular intervals.\n",
    "   - Save each patch as a `.png` image.\n",
    "   - Record metadata for each patch:\n",
    "     - Slide ID (origin WSI)\n",
    "     - Patch filename\n",
    "     - Top-left (X, Y) coordinate of the patch within the slide\n",
    "     - Path where the patch is stored\n",
    "\n",
    "4. **Handling errors**:\n",
    "   - If a WSI fails to load or process, it is added to `failed_wsi_files`.\n",
    "\n",
    "5. **Metadata saving**:\n",
    "   - After processing all WSIs, the collected metadata is stored in a single CSV file (`WSI_patch_metadata_prostate.csv`).\n",
    "\n",
    "---\n",
    "\n",
    "The metadata will also be important for linking back embeddings to their original spatial locations on the WSI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m wsi_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(wsi_folder, wsi_file)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m     wsi_image \u001b[38;5;241m=\u001b[39m \u001b[43mtifffile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwsi_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     wsi_image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(wsi_image)  \n\u001b[1;32m     21\u001b[0m     slide_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(wsi_file)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/UNI/lib/python3.10/site-packages/tifffile/tifffile.py:1235\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(files, selection, aszarr, key, series, level, squeeze, maxworkers, buffersize, mode, name, offset, size, pattern, axesorder, categories, imread, imreadargs, sort, container, chunkshape, chunkdtype, axestiled, ioworkers, chunkmode, fillvalue, zattrs, multiscales, omexml, out, out_inplace, _multifile, _useframes, **kwargs)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m store\n\u001b[1;32m   1234\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m zarr_selection(store, selection, out\u001b[38;5;241m=\u001b[39mout)\n\u001b[0;32m-> 1235\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtif\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m                \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m                \u001b[49m\u001b[43mseries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m                \u001b[49m\u001b[43msqueeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmaxworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbuffersize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffersize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m                \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(files, (FileHandle, IO)):\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBinaryIO not supported\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/UNI/lib/python3.10/site-packages/tifffile/tifffile.py:4495\u001b[0m, in \u001b[0;36mTiffFile.asarray\u001b[0;34m(self, key, series, level, squeeze, out, maxworkers, buffersize)\u001b[0m\n\u001b[1;32m   4493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m page0 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4494\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage is None\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 4495\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpage0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffersize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffersize\u001b[49m\n\u001b[1;32m   4497\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4498\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4499\u001b[0m     result \u001b[38;5;241m=\u001b[39m stack_pages(\n\u001b[1;32m   4500\u001b[0m         pages, out\u001b[38;5;241m=\u001b[39mout, maxworkers\u001b[38;5;241m=\u001b[39mmaxworkers, buffersize\u001b[38;5;241m=\u001b[39mbuffersize\n\u001b[1;32m   4501\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/UNI/lib/python3.10/site-packages/tifffile/tifffile.py:8831\u001b[0m, in \u001b[0;36mTiffPage.asarray\u001b[0;34m(self, out, squeeze, lock, maxworkers, buffersize)\u001b[0m\n\u001b[1;32m   8821\u001b[0m             out[\n\u001b[1;32m   8822\u001b[0m                 s, d : d \u001b[38;5;241m+\u001b[39m shape[\u001b[38;5;241m0\u001b[39m], h : h \u001b[38;5;241m+\u001b[39m shape[\u001b[38;5;241m1\u001b[39m], w : w \u001b[38;5;241m+\u001b[39m shape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m   8823\u001b[0m             ] \u001b[38;5;241m=\u001b[39m segment[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   8826\u001b[0m                 : keyframe\u001b[38;5;241m.\u001b[39mimagewidth \u001b[38;5;241m-\u001b[39m w,\n\u001b[1;32m   8827\u001b[0m             ]\n\u001b[1;32m   8828\u001b[0m         \u001b[38;5;66;03m# except IndexError:\u001b[39;00m\n\u001b[1;32m   8829\u001b[0m         \u001b[38;5;66;03m#     pass  # corrupted file, for example, with too many strips\u001b[39;00m\n\u001b[0;32m-> 8831\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegments(\n\u001b[1;32m   8832\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   8833\u001b[0m         lock\u001b[38;5;241m=\u001b[39mlock,\n\u001b[1;32m   8834\u001b[0m         maxworkers\u001b[38;5;241m=\u001b[39mmaxworkers,\n\u001b[1;32m   8835\u001b[0m         buffersize\u001b[38;5;241m=\u001b[39mbuffersize,\n\u001b[1;32m   8836\u001b[0m         sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   8837\u001b[0m         _fullsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   8838\u001b[0m     ):\n\u001b[1;32m   8839\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   8841\u001b[0m result\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m=\u001b[39m keyframe\u001b[38;5;241m.\u001b[39mshaped\n",
      "File \u001b[0;32m~/anaconda3/envs/UNI/lib/python3.10/site-packages/tifffile/tifffile.py:8645\u001b[0m, in \u001b[0;36mTiffPage.segments\u001b[0;34m(self, lock, maxworkers, func, sort, buffersize, _fullsize)\u001b[0m\n\u001b[1;32m   8636\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(maxworkers) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m   8637\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m segments \u001b[38;5;129;01min\u001b[39;00m fh\u001b[38;5;241m.\u001b[39mread_segments(\n\u001b[1;32m   8638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataoffsets,\n\u001b[1;32m   8639\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatabytecounts,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   8643\u001b[0m         flat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   8644\u001b[0m     ):\n\u001b[0;32m-> 8645\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m executor\u001b[38;5;241m.\u001b[39mmap(decode, segments)\n",
      "File \u001b[0;32m~/anaconda3/envs/UNI/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m~/anaconda3/envs/UNI/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m~/anaconda3/envs/UNI/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/anaconda3/envs/UNI/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "patch_size = (224, 224)  \n",
    "stride = 224  \n",
    "root_path = output_base_folder\n",
    "\n",
    "\n",
    "wsi_files = [f for f in os.listdir(wsi_folder) if f.endswith(\".tif\")]\n",
    "failed_wsi_files = []\n",
    "all_patch_metadata = []  \n",
    "\n",
    "for wsi_file in wsi_files:\n",
    "    wsi_path = os.path.join(wsi_folder, wsi_file)\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        wsi_image = tifffile.imread(wsi_path)\n",
    "        \n",
    "      \n",
    "        wsi_image = Image.fromarray(wsi_image)  \n",
    "\n",
    "  \n",
    "        slide_name = os.path.splitext(wsi_file)[0]\n",
    "        slide_output_folder = os.path.join(output_base_folder, slide_name)\n",
    "        os.makedirs(slide_output_folder, exist_ok=True)\n",
    "\n",
    "    \n",
    "        wsi_width, wsi_height = wsi_image.size\n",
    "        print(f\"Processing {slide_name} - Size: {wsi_width}x{wsi_height}\")\n",
    "\n",
    "        saved_patches = 0\n",
    "        for x in range(0, wsi_width - patch_size[0], stride):\n",
    "            for y in range(0, wsi_height - patch_size[1], stride):\n",
    "                patch = wsi_image.crop((x, y, x + patch_size[0], y + patch_size[1]))\n",
    "\n",
    "               \n",
    "               \n",
    "                patch_filename = f\"patch_{saved_patches+1}.png\"\n",
    "                patch_path = os.path.join(slide_output_folder, patch_filename)\n",
    "                patch.save(patch_path)\n",
    "\n",
    "                \n",
    "                all_patch_metadata.append([slide_name, patch_filename, x, y, patch_path])\n",
    "                saved_patches += 1\n",
    "\n",
    "        print(f\"Saved {saved_patches} patches for {slide_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {wsi_file}: {e}\")\n",
    "        failed_wsi_files.append(wsi_file)\n",
    "\n",
    "\n",
    "metadata_df = pd.DataFrame(all_patch_metadata, columns=['Slide_ID', 'Patch_ID', 'X', 'Y', 'Path'])\n",
    "metadata_df.to_csv(metadata_file, index=False)\n",
    "\n",
    "print(\"All WSIs processed successfully!\")\n",
    "print(\"Failed WSIs:\", failed_wsi_files)\n",
    "print(f\"Metadata saved in {metadata_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Patch Feature Embeddings Using the UNI Model\n",
    "\n",
    "In this section, we load the extracted patches, pass them through the pretrained UNI model, and save their feature embeddings along with spatial metadata.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. **Patch Dataset Preparation**:\n",
    "   - Define a `PatchDataset` class that:\n",
    "     - Loads all `.png` patch images from a given folder.\n",
    "     - Applies the same image transformations we defined earlier.\n",
    "     - Returns an image and a dummy label (not used here).\n",
    "\n",
    "2. **Loading Patches and Creating DataLoader**:\n",
    "   - For each WSI (each folder of patches):\n",
    "     - Create a `PatchDataset` and corresponding `DataLoader`.\n",
    "     - Load patches in small batches (batch size = 4) without shuffling (preserve patch order).\n",
    "\n",
    "3. **Extract Patch Features**:\n",
    "   - Use the `extract_patch_features_from_dataloader` function from UNI to pass the patches through the model.\n",
    "   - Extract the feature embeddings for each patch.\n",
    "\n",
    "4. **Collect Embeddings and Metadata**:\n",
    "   - For each patch, store:\n",
    "     - Its extracted embedding.\n",
    "     - The corresponding Slide_ID (WSI name).\n",
    "     - The Patch_ID (patch filename).\n",
    "\n",
    "5. **Merge with Patch Location Metadata**:\n",
    "   - Combine the extracted embeddings with the previously saved patch spatial information (`X`, `Y` coordinates) using a `match_id` (Slide_ID + Patch_ID).\n",
    "\n",
    "6. **Save Final Data**:\n",
    "   - Store everything in a CSV file `WSI_patch_embeddings_prostate.csv`, which contains:\n",
    "     - Patch embeddings (feature vectors)\n",
    "     - Slide_ID\n",
    "     - Patch_ID\n",
    "     - X, Y coordinates on the original WSI\n",
    "\n",
    "---\n",
    "\n",
    "At the end of this step, we will have a **structured table** linking each patch's location to its **deep feature representation**, ready for downstream spatial analysis tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/3416 [00:36<6:59:14,  7.37s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 44\u001b[0m\n\u001b[1;32m     40\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m PatchDataset(root_dir\u001b[38;5;241m=\u001b[39mfolder_path, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[1;32m     41\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 44\u001b[0m test_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_patch_features_from_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m test_feats \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(test_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     48\u001b[0m all_embeddings\u001b[38;5;241m.\u001b[39mappend(test_feats\u001b[38;5;241m.\u001b[39mnumpy())  \u001b[38;5;66;03m# Convert to NumPy\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/UNI/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/RWTH/biolab/bioInfoLab/generate_embeddings/UNI/uni/downstream/extract_patch_features.py:38\u001b[0m, in \u001b[0;36mextract_patch_features_from_dataloader\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m     36\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m---> 38\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()[:remaining, :]\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     39\u001b[0m     labels \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mnumpy()[:remaining]\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(embeddings)\u001b[38;5;241m.\u001b[39many()\n",
      "File \u001b[0;32m~/anaconda3/envs/UNI/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/UNI/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/UNI/lib/python3.10/site-packages/timm/models/vision_transformer.py:680\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 680\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/UNI/lib/python3.10/site-packages/timm/models/vision_transformer.py:664\u001b[0m, in \u001b[0;36mVisionTransformer.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    662\u001b[0m     x \u001b[38;5;241m=\u001b[39m checkpoint_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, x)\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 664\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/UNI/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/UNI/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/UNI/lib/python3.10/site-packages/torch/nn/modules/container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/UNI/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/UNI/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/UNI/lib/python3.10/site-packages/timm/models/vision_transformer.py:156\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    155\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x))))\n\u001b[0;32m--> 156\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/UNI/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/UNI/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/UNI/lib/python3.10/site-packages/timm/layers/mlp.py:91\u001b[0m, in \u001b[0;36mGluMlp.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 91\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     x1, x2 \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_dim)\n\u001b[1;32m     93\u001b[0m     x \u001b[38;5;241m=\u001b[39m x1 \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x2) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_last \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x1) \u001b[38;5;241m*\u001b[39m x2\n",
      "File \u001b[0;32m~/anaconda3/envs/UNI/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/UNI/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/UNI/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from uni.downstream.extract_patch_features import extract_patch_features_from_dataloader\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.image_paths = glob.glob(os.path.join(root_dir, \"*.png\"))  # Load all PNGs\n",
    "        self.transform = transform\n",
    "        self.patch_ids = [os.path.basename(img) for img in self.image_paths]  # Store Patch_IDs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")  # Ensure 3 channels\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, -1  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "valid_folders = [f for f in sorted(os.listdir(root_path)) if os.path.isdir(os.path.join(root_path, f)) and not f.startswith(\".\")]\n",
    "\n",
    "all_embeddings = []\n",
    "all_sample_ids = []\n",
    "all_patch_ids = []  \n",
    "\n",
    "for folder in valid_folders:\n",
    "    folder_path = os.path.join(root_path, folder)\n",
    "\n",
    "    test_dataset = PatchDataset(root_dir=folder_path, transform=transform)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "   \n",
    "    test_features = extract_patch_features_from_dataloader(model, test_dataloader)\n",
    "    test_feats = torch.Tensor(test_features['embeddings'])\n",
    "\n",
    "    \n",
    "    all_embeddings.append(test_feats.numpy())  # Convert to NumPy\n",
    "    all_sample_ids.extend([folder] * test_feats.shape[0])  # Assign folder name to each patch\n",
    "    all_patch_ids.extend(test_dataset.patch_ids)  \n",
    "\n",
    "\n",
    "df_embeddings = pd.DataFrame(\n",
    "    torch.cat([torch.tensor(arr) for arr in all_embeddings], dim=0).numpy()  # Flatten list of arrays\n",
    ")\n",
    "df_embeddings[\"Slide_ID\"] = all_sample_ids \n",
    "df_embeddings[\"Patch_ID\"] = all_patch_ids  \n",
    "\n",
    "\n",
    "df_embeddings['match_id']=df_embeddings['Slide_ID']+'_'+df_embeddings['Patch_ID']\n",
    "metadata_df['match_id']=metadata_df['Slide_ID']+'_'+metadata_df['Patch_ID']\n",
    "df_embeddings=df_embeddings.merge(metadata_df[['match_id','X','Y','Patch_ID']],on='match_id')\n",
    "df_embeddings.to_csv(embedding_file,index=False) #final file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UNI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
